{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn import preprocessing, impute\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Load training feats\n",
    "train = pd.read_hdf('../data/train_feats_od.h5')\n",
    "\n",
    "# Load tgt and add to feats\n",
    "train['tgt'] = np.load('../data/target_dummy.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hostgal_photoz</th>\n",
       "      <th>flux_mean_0</th>\n",
       "      <th>flux_mean_5</th>\n",
       "      <th>flux_min_1</th>\n",
       "      <th>flux_min_2</th>\n",
       "      <th>flux_min_5</th>\n",
       "      <th>flux_std_0</th>\n",
       "      <th>flux_std_1</th>\n",
       "      <th>flux_std_2</th>\n",
       "      <th>flux_std_5</th>\n",
       "      <th>...</th>\n",
       "      <th>abs_magnitude_max_5</th>\n",
       "      <th>absmagmax_ratio_bands_2_3</th>\n",
       "      <th>absmagmax_ratio_bands_2_4</th>\n",
       "      <th>absmagmax_ratio_bands_2_5</th>\n",
       "      <th>absmagmax_ratio_bands_3_4</th>\n",
       "      <th>absmagmax_ratio_bands_3_5</th>\n",
       "      <th>absmagmax_ratio_bands_4_5</th>\n",
       "      <th>spike_back_mean</th>\n",
       "      <th>spike_front_mean</th>\n",
       "      <th>tgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>-10.051806</td>\n",
       "      <td>-54.414902</td>\n",
       "      <td>-1100.440063</td>\n",
       "      <td>-681.858887</td>\n",
       "      <td>-422.815094</td>\n",
       "      <td>83.275841</td>\n",
       "      <td>596.576904</td>\n",
       "      <td>451.180817</td>\n",
       "      <td>292.182281</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17.451452</td>\n",
       "      <td>107.970627</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.6267</td>\n",
       "      <td>-3.119513</td>\n",
       "      <td>-1.848958</td>\n",
       "      <td>-11.715749</td>\n",
       "      <td>-10.067919</td>\n",
       "      <td>-14.211164</td>\n",
       "      <td>7.062516</td>\n",
       "      <td>5.661101</td>\n",
       "      <td>5.718981</td>\n",
       "      <td>7.030447</td>\n",
       "      <td>...</td>\n",
       "      <td>-48.329815</td>\n",
       "      <td>0.998342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.992395</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.994043</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.357979</td>\n",
       "      <td>1.904276</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.2262</td>\n",
       "      <td>-0.042413</td>\n",
       "      <td>4.834683</td>\n",
       "      <td>-3.393080</td>\n",
       "      <td>-2.848838</td>\n",
       "      <td>-19.159811</td>\n",
       "      <td>1.816127</td>\n",
       "      <td>1.789767</td>\n",
       "      <td>5.505767</td>\n",
       "      <td>13.201397</td>\n",
       "      <td>...</td>\n",
       "      <td>-44.443485</td>\n",
       "      <td>0.988435</td>\n",
       "      <td>0.983498</td>\n",
       "      <td>0.980152</td>\n",
       "      <td>0.995005</td>\n",
       "      <td>0.991620</td>\n",
       "      <td>0.996598</td>\n",
       "      <td>1.175826</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.2813</td>\n",
       "      <td>1.479138</td>\n",
       "      <td>9.389122</td>\n",
       "      <td>-3.618410</td>\n",
       "      <td>-2.159753</td>\n",
       "      <td>-10.249387</td>\n",
       "      <td>4.343960</td>\n",
       "      <td>25.731789</td>\n",
       "      <td>31.671373</td>\n",
       "      <td>25.822132</td>\n",
       "      <td>...</td>\n",
       "      <td>-46.172092</td>\n",
       "      <td>1.001930</td>\n",
       "      <td>1.004307</td>\n",
       "      <td>1.010460</td>\n",
       "      <td>1.002372</td>\n",
       "      <td>1.008514</td>\n",
       "      <td>1.006127</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.029715</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.2415</td>\n",
       "      <td>0.747304</td>\n",
       "      <td>6.299269</td>\n",
       "      <td>-2.622109</td>\n",
       "      <td>-2.084535</td>\n",
       "      <td>-10.860540</td>\n",
       "      <td>2.341279</td>\n",
       "      <td>8.037329</td>\n",
       "      <td>21.135263</td>\n",
       "      <td>21.245771</td>\n",
       "      <td>...</td>\n",
       "      <td>-45.511734</td>\n",
       "      <td>0.993583</td>\n",
       "      <td>0.992954</td>\n",
       "      <td>0.999450</td>\n",
       "      <td>0.999367</td>\n",
       "      <td>1.005906</td>\n",
       "      <td>1.006542</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.062956</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   hostgal_photoz  flux_mean_0  flux_mean_5   flux_min_1  flux_min_2  \\\n",
       "0          0.0000   -10.051806   -54.414902 -1100.440063 -681.858887   \n",
       "1          1.6267    -3.119513    -1.848958   -11.715749  -10.067919   \n",
       "2          0.2262    -0.042413     4.834683    -3.393080   -2.848838   \n",
       "3          0.2813     1.479138     9.389122    -3.618410   -2.159753   \n",
       "4          0.2415     0.747304     6.299269    -2.622109   -2.084535   \n",
       "\n",
       "   flux_min_5  flux_std_0  flux_std_1  flux_std_2  flux_std_5 ...   \\\n",
       "0 -422.815094   83.275841  596.576904  451.180817  292.182281 ...    \n",
       "1  -14.211164    7.062516    5.661101    5.718981    7.030447 ...    \n",
       "2  -19.159811    1.816127    1.789767    5.505767   13.201397 ...    \n",
       "3  -10.249387    4.343960   25.731789   31.671373   25.822132 ...    \n",
       "4  -10.860540    2.341279    8.037329   21.135263   21.245771 ...    \n",
       "\n",
       "   abs_magnitude_max_5  absmagmax_ratio_bands_2_3  absmagmax_ratio_bands_2_4  \\\n",
       "0                  NaN                        NaN                        NaN   \n",
       "1           -48.329815                   0.998342                        NaN   \n",
       "2           -44.443485                   0.988435                   0.983498   \n",
       "3           -46.172092                   1.001930                   1.004307   \n",
       "4           -45.511734                   0.993583                   0.992954   \n",
       "\n",
       "   absmagmax_ratio_bands_2_5  absmagmax_ratio_bands_3_4  \\\n",
       "0                        NaN                        NaN   \n",
       "1                   0.992395                        NaN   \n",
       "2                   0.980152                   0.995005   \n",
       "3                   1.010460                   1.002372   \n",
       "4                   0.999450                   0.999367   \n",
       "\n",
       "   absmagmax_ratio_bands_3_5  absmagmax_ratio_bands_4_5  spike_back_mean  \\\n",
       "0                        NaN                        NaN        17.451452   \n",
       "1                   0.994043                        NaN         2.357979   \n",
       "2                   0.991620                   0.996598         1.175826   \n",
       "3                   1.008514                   1.006127              NaN   \n",
       "4                   1.005906                   1.006542              NaN   \n",
       "\n",
       "   spike_front_mean  tgt  \n",
       "0        107.970627   92  \n",
       "1          1.904276   88  \n",
       "2          1.000000   42  \n",
       "3          1.029715   90  \n",
       "4          1.062956   90  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90    2313\n",
       "42    1193\n",
       "65     981\n",
       "16     924\n",
       "15     495\n",
       "62     484\n",
       "88     370\n",
       "92     239\n",
       "67     208\n",
       "52     183\n",
       "95     175\n",
       "6      151\n",
       "64     102\n",
       "53      30\n",
       "Name: tgt, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['tgt'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> train, threshold -1.65 : Precision = nan, Recall = 0.00\n",
      "> train, threshold -1.48 : Precision = nan, Recall = 0.00\n",
      "> train, threshold -1.31 : Precision = nan, Recall = 0.00\n",
      "> train, threshold -1.14 : Precision = nan, Recall = 0.00\n",
      "> train, threshold -0.96 : Precision = nan, Recall = 0.00\n",
      "> train, threshold -0.79 : Precision = nan, Recall = 0.00\n",
      "> train, threshold -0.62 : Precision = nan, Recall = 0.00\n",
      "> train, threshold -0.45 : Precision = nan, Recall = 0.00\n",
      "> train, threshold -0.28 : Precision = nan, Recall = 0.00\n",
      "> train, threshold -0.11 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 0.06 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 0.23 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 0.41 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 0.58 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 0.75 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 0.92 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 1.09 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 1.26 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 1.43 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 1.60 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 1.78 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 1.95 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 2.12 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 2.29 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 2.46 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 2.63 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 2.80 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 2.97 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 3.15 : Precision = nan, Recall = 0.00\n",
      "> train, threshold 3.32 : Precision = nan, Recall = nan\n",
      "> test, threshold -2.22 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -2.03 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -1.84 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -1.65 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -1.46 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -1.27 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -1.08 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -0.89 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -0.70 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -0.50 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -0.31 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold -0.12 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold 0.07 : Precision = 0.00, Recall = 0.00\n",
      "> test, threshold 0.26 : Precision = 0.05, Recall = 0.00\n",
      "> test, threshold 0.45 : Precision = 0.05, Recall = 0.00\n",
      "> test, threshold 0.64 : Precision = 0.05, Recall = 0.00\n",
      "> test, threshold 0.83 : Precision = 0.05, Recall = 0.00\n",
      "> test, threshold 1.02 : Precision = 0.05, Recall = 0.00\n",
      "> test, threshold 1.21 : Precision = 0.05, Recall = 0.00\n",
      "> test, threshold 1.40 : Precision = 0.18, Recall = 0.01\n",
      "> test, threshold 1.59 : Precision = 0.32, Recall = 0.01\n",
      "> test, threshold 1.78 : Precision = 0.36, Recall = 0.01\n",
      "> test, threshold 1.97 : Precision = 0.50, Recall = 0.02\n",
      "> test, threshold 2.16 : Precision = 0.73, Recall = 0.03\n",
      "> test, threshold 2.36 : Precision = 0.77, Recall = 0.04\n",
      "> test, threshold 2.55 : Precision = 0.86, Recall = 0.06\n",
      "> test, threshold 2.74 : Precision = 1.00, Recall = 0.12\n",
      "> test, threshold 2.93 : Precision = 1.00, Recall = 0.24\n",
      "> test, threshold 3.12 : Precision = 1.00, Recall = 0.50\n",
      "> test, threshold 3.31 : Precision = 1.00, Recall = 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/miguelrita/anaconda2/envs/titanic/lib/python3.6/site-packages/ipykernel_launcher.py:88: RuntimeWarning: invalid value encountered in long_scalars\n",
      "/Users/miguelrita/anaconda2/envs/titanic/lib/python3.6/site-packages/ipykernel_launcher.py:89: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    }
   ],
   "source": [
    "# Lets first try novelty detection on class 95 eg.\n",
    "\n",
    "# First, build the training and test sets from one start kfold\n",
    "y_tgt = train['tgt'].values\n",
    "num_folds = 8\n",
    "folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=1)\n",
    "_train, _eval = next(folds.split(y_tgt, y_tgt))\n",
    "test_X = train.iloc[_eval]\n",
    "\n",
    "# Build train data - test class eg. 95 (will remain out of train data)\n",
    "TEST_CLASS = 95\n",
    "train_X = train.iloc[_train]\n",
    "pure_mask = train_X['tgt']!=TEST_CLASS\n",
    "train_X = train_X[pure_mask]\n",
    "\n",
    "# Get balanced weights\n",
    "w = compute_sample_weight('balanced', y_tgt)\n",
    "train_w = w[_train]\n",
    "train_w = train_w[pure_mask]\n",
    "\n",
    "# Preprocessing\n",
    "X_train = train_X.values[:,:-1]\n",
    "X_test = test_X.values[:,:-1]\n",
    "\n",
    "# Scale\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Impute\n",
    "imp = impute.SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "imp.fit(X_train)\n",
    "X_train = imp.transform(X_train)\n",
    "X_test = imp.transform(X_test)\n",
    "\n",
    "# Fit oneclass SVM\n",
    "nu = train['tgt'].value_counts()[TEST_CLASS] / train.shape[0]\n",
    "clf = svm.OneClassSVM(nu=nu, kernel='rbf', gamma='auto')\n",
    "\n",
    "clf.fit(X_train, sample_weight=train_w)\n",
    "\n",
    "# Predict on train and test set\n",
    "y_pred_train = clf.predict(X_train)\n",
    "y_pred_test = clf.predict(X_test)\n",
    "\n",
    "# Setup target vectors\n",
    "y_train = np.copy(train_X.values[:,-1])\n",
    "y_train[y_train==TEST_CLASS] = -1\n",
    "y_train[np.logical_and(y_train!=TEST_CLASS, y_train!=-1)] = 1\n",
    "\n",
    "y_test = np.copy(test_X.values[:,-1])\n",
    "y_test[y_test==TEST_CLASS] = -1\n",
    "y_test[np.logical_and(y_test!=TEST_CLASS, y_test!=-1)] = 1\n",
    "\n",
    "# Compute precision / recall for train and test, at different thresholds\n",
    "\n",
    "def get_stats(y_true, y_pred, pos_label):\n",
    "    '''\n",
    "    return tp, fp, fn, tn tuple\n",
    "    '''\n",
    "    \n",
    "    pmask = y_true == pos_label # Positive mask on truth\n",
    "    nmask = np.logical_not(pmask) # negative mask on truth\n",
    "    \n",
    "    tp = np.sum(y_true[pmask] == y_pred[pmask])\n",
    "    fp = np.sum(y_true[pmask] != y_pred[pmask])\n",
    "    tn = np.sum(y_true[nmask] == y_pred[nmask])\n",
    "    fn = np.sum(y_true[nmask] != y_pred[nmask])\n",
    "    \n",
    "    return tp, fp, fn ,tn\n",
    "\n",
    "for name, iset, truth in zip(['train', 'test'], [X_train, X_test], [y_train, y_test]):\n",
    "    \n",
    "    # Get scoring function\n",
    "    y_pred_raw = clf.decision_function(iset)\n",
    "    \n",
    "    # Compute precision / recall for each thresh\n",
    "    for thresh in np.linspace(np.min(y_pred_raw), np.max(y_pred_raw), 30):\n",
    "        \n",
    "        y_pred = np.copy(y_pred_raw)\n",
    "        y_pred[y_pred_raw <= thresh] = -1\n",
    "        y_pred[y_pred_raw > thresh] = 1\n",
    "        \n",
    "        # Get stats\n",
    "        tp, fp, tn, fn = get_stats(truth, y_pred, pos_label=-1)\n",
    "        \n",
    "        # Compute precision and recall\n",
    "        prec = tp / (tp + fp)\n",
    "        recall = tp / (tp + fn)\n",
    "        \n",
    "        # Print results\n",
    "        print(f'> {name}, threshold {thresh:.2f} : Precision = {prec:.2f}, Recall = {recall:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.02831403, 0.02706186, 0.02709677, 0.02713178, 0.02716688,\n",
       "       0.02720207, 0.02723735, 0.02727273, 0.02730819, 0.02734375,\n",
       "       0.0273794 , 0.02741514, 0.02745098, 0.02748691, 0.02752294,\n",
       "       0.02755906, 0.02759527, 0.02631579, 0.02635046, 0.02638522,\n",
       "       0.02642008, 0.02645503, 0.02649007, 0.0265252 , 0.02656042,\n",
       "       0.02659574, 0.02663116, 0.02666667, 0.02670227, 0.02673797,\n",
       "       0.02677376, 0.02680965, 0.02684564, 0.02688172, 0.0269179 ,\n",
       "       0.02695418, 0.02699055, 0.02702703, 0.0270636 , 0.02710027,\n",
       "       0.02713704, 0.02717391, 0.02721088, 0.02724796, 0.02728513,\n",
       "       0.0273224 , 0.02735978, 0.02739726, 0.02743484, 0.02747253,\n",
       "       0.02751032, 0.02754821, 0.02758621, 0.02762431, 0.02766252,\n",
       "       0.02770083, 0.02773925, 0.02777778, 0.02781641, 0.02785515,\n",
       "       0.027894  , 0.02793296, 0.02797203, 0.0280112 , 0.02805049,\n",
       "       0.02808989, 0.0281294 , 0.02816901, 0.02820874, 0.02824859,\n",
       "       0.02828854, 0.02832861, 0.02836879, 0.02840909, 0.0284495 ,\n",
       "       0.02849003, 0.02853067, 0.02714286, 0.02718169, 0.02722063,\n",
       "       0.02725968, 0.02729885, 0.02733813, 0.02737752, 0.02741703,\n",
       "       0.02745665, 0.02749638, 0.02753623, 0.0275762 , 0.02761628,\n",
       "       0.02765648, 0.02769679, 0.02773723, 0.02777778, 0.02781845,\n",
       "       0.02785924, 0.02790015, 0.02794118, 0.02798233, 0.0280236 ,\n",
       "       0.02806499, 0.02810651, 0.02814815, 0.02818991, 0.0282318 ,\n",
       "       0.02827381, 0.02831595, 0.02835821, 0.0284006 , 0.02844311,\n",
       "       0.02848576, 0.02852853, 0.02857143, 0.02861446, 0.02865762,\n",
       "       0.02870091, 0.02874433, 0.02878788, 0.02883156, 0.02887538,\n",
       "       0.02891933, 0.02896341, 0.02900763, 0.02905199, 0.02909648,\n",
       "       0.0291411 , 0.02918587, 0.02923077, 0.02927581, 0.02932099,\n",
       "       0.02936631, 0.02941176, 0.02945736, 0.02950311, 0.02954899,\n",
       "       0.02959502, 0.02964119, 0.0296875 , 0.02973396, 0.02978056,\n",
       "       0.02982732, 0.02987421, 0.02992126, 0.02996845, 0.0300158 ,\n",
       "       0.03006329, 0.03011094, 0.03015873, 0.03020668, 0.03025478,\n",
       "       0.03030303, 0.03035144, 0.0304    , 0.03044872, 0.03049759,\n",
       "       0.03054662, 0.02898551, 0.02903226, 0.02907916, 0.02912621,\n",
       "       0.02917342, 0.02922078, 0.02926829, 0.02931596, 0.02936378,\n",
       "       0.02941176, 0.0294599 , 0.0295082 , 0.02955665, 0.02960526,\n",
       "       0.02965404, 0.02970297, 0.02809917, 0.0281457 , 0.02819237,\n",
       "       0.0282392 , 0.02828619, 0.02833333, 0.02838063, 0.02842809,\n",
       "       0.02847571, 0.02852349, 0.02857143, 0.02861953, 0.02866779,\n",
       "       0.02871622, 0.02876481, 0.02881356, 0.02886248, 0.02891156,\n",
       "       0.02896082, 0.02901024, 0.02905983, 0.02910959, 0.02915952,\n",
       "       0.02920962, 0.0292599 , 0.02931034, 0.02936097, 0.02941176,\n",
       "       0.02946274, 0.02951389, 0.02956522, 0.02961672, 0.02966841,\n",
       "       0.02972028, 0.02977233, 0.02982456, 0.02987698, 0.02992958,\n",
       "       0.02998236, 0.03003534, 0.0300885 , 0.02836879, 0.02841918,\n",
       "       0.02846975, 0.0285205 , 0.02857143, 0.02862254, 0.02867384,\n",
       "       0.02872531, 0.02877698, 0.02882883, 0.02888087, 0.02893309,\n",
       "       0.02898551, 0.02903811, 0.02909091, 0.0291439 , 0.02919708,\n",
       "       0.02925046, 0.02930403, 0.0293578 , 0.02941176, 0.02946593,\n",
       "       0.0295203 , 0.02957486, 0.02962963, 0.0296846 , 0.02973978,\n",
       "       0.02979516, 0.02985075, 0.02990654, 0.02996255, 0.03001876,\n",
       "       0.03007519, 0.03013183, 0.03018868, 0.03024575, 0.03030303,\n",
       "       0.03036053, 0.03041825, 0.03047619, 0.03053435, 0.03059273,\n",
       "       0.03065134, 0.03071017, 0.03076923, 0.03082852, 0.03088803,\n",
       "       0.03094778, 0.03100775, 0.03106796, 0.0311284 , 0.03118908,\n",
       "       0.03125   , 0.03131115, 0.03137255, 0.03143418, 0.03149606,\n",
       "       0.03155819, 0.03162055, 0.03168317, 0.03174603, 0.03180915,\n",
       "       0.03187251, 0.03193613, 0.032     , 0.03206413, 0.03212851,\n",
       "       0.03219316, 0.03225806, 0.03232323, 0.03238866, 0.03245436,\n",
       "       0.03252033, 0.03258656, 0.03265306, 0.03271984, 0.03278689,\n",
       "       0.03285421, 0.03292181, 0.03298969, 0.03305785, 0.03312629,\n",
       "       0.03319502, 0.03326403, 0.03333333, 0.03340292, 0.0334728 ,\n",
       "       0.03354298, 0.03361345, 0.03368421, 0.03375527, 0.03382664,\n",
       "       0.03389831, 0.03397028, 0.03404255, 0.03411514, 0.03418803,\n",
       "       0.03211991, 0.03218884, 0.03225806, 0.03232759, 0.03239741,\n",
       "       0.03246753, 0.03253796, 0.0326087 , 0.03267974, 0.03275109,\n",
       "       0.03282276, 0.03289474, 0.03296703, 0.03303965, 0.03311258,\n",
       "       0.03318584, 0.03325942, 0.03333333, 0.03340757, 0.03348214,\n",
       "       0.03355705, 0.03363229, 0.03370787, 0.03378378, 0.03386005,\n",
       "       0.03393665, 0.03401361, 0.03409091, 0.03416856, 0.03424658,\n",
       "       0.03432494, 0.03440367, 0.03218391, 0.03225806, 0.03233256,\n",
       "       0.03240741, 0.0324826 , 0.03255814, 0.03263403, 0.03271028,\n",
       "       0.03278689, 0.03286385, 0.03294118, 0.03301887, 0.03309693,\n",
       "       0.03317536, 0.03325416, 0.03333333, 0.03341289, 0.03349282,\n",
       "       0.03357314, 0.03365385, 0.03373494, 0.03381643, 0.03389831,\n",
       "       0.03398058, 0.03406326, 0.03414634, 0.03422983, 0.03431373,\n",
       "       0.03439803, 0.03448276, 0.0345679 , 0.03465347, 0.03473945,\n",
       "       0.03233831, 0.03241895, 0.0325    , 0.03258145, 0.03015075,\n",
       "       0.0302267 , 0.03030303, 0.03037975, 0.03045685, 0.03053435,\n",
       "       0.03061224, 0.03069054, 0.03076923, 0.03084833, 0.02835052,\n",
       "       0.02842377, 0.02849741, 0.02857143, 0.02864583, 0.02872063,\n",
       "       0.02879581, 0.02887139, 0.02894737, 0.02902375, 0.02910053,\n",
       "       0.02917772, 0.02925532, 0.02933333, 0.02941176, 0.02949062,\n",
       "       0.02956989, 0.0296496 , 0.02972973, 0.0298103 , 0.0298913 ,\n",
       "       0.02997275, 0.03005464, 0.02739726, 0.02747253, 0.02754821,\n",
       "       0.02762431, 0.02770083, 0.02777778, 0.02506964, 0.02513966,\n",
       "       0.02521008, 0.0252809 , 0.02535211, 0.02542373, 0.02549575,\n",
       "       0.02556818, 0.02564103, 0.02571429, 0.02578797, 0.02586207,\n",
       "       0.0259366 , 0.02601156, 0.02608696, 0.02616279, 0.02623907,\n",
       "       0.02631579, 0.02639296, 0.02647059, 0.02654867, 0.02662722,\n",
       "       0.02670623, 0.02678571, 0.02686567, 0.02694611, 0.02702703,\n",
       "       0.02710843, 0.02719033, 0.02727273, 0.02431611, 0.02439024,\n",
       "       0.02446483, 0.02453988, 0.02461538, 0.02469136, 0.0247678 ,\n",
       "       0.02484472, 0.02492212, 0.025     , 0.02507837, 0.02515723,\n",
       "       0.02523659, 0.02531646, 0.02539683, 0.02547771, 0.02555911,\n",
       "       0.02564103, 0.02572347, 0.02580645, 0.02588997, 0.02597403,\n",
       "       0.02605863, 0.02614379, 0.02622951, 0.02631579, 0.02640264,\n",
       "       0.02649007, 0.02657807, 0.02666667, 0.02675585, 0.02684564,\n",
       "       0.02693603, 0.02702703, 0.02711864, 0.02721088, 0.02730375,\n",
       "       0.02739726, 0.02749141, 0.02758621, 0.02768166, 0.02777778,\n",
       "       0.02787456, 0.02797203, 0.02807018, 0.02816901, 0.02826855,\n",
       "       0.02836879, 0.02846975, 0.02857143, 0.02867384, 0.02877698,\n",
       "       0.02888087, 0.02898551, 0.02909091, 0.02919708, 0.02930403,\n",
       "       0.02941176, 0.02583026, 0.02592593, 0.0260223 , 0.0261194 ,\n",
       "       0.02621723, 0.02631579, 0.02641509, 0.02651515, 0.02661597,\n",
       "       0.02671756, 0.02681992, 0.02692308, 0.02702703, 0.02713178,\n",
       "       0.02723735, 0.02734375, 0.02745098, 0.02755906, 0.02371542,\n",
       "       0.02380952, 0.02390438, 0.024     , 0.02409639, 0.02419355,\n",
       "       0.0242915 , 0.02439024, 0.0244898 , 0.02459016, 0.02469136,\n",
       "       0.02479339, 0.02489627, 0.025     , 0.0251046 , 0.02521008,\n",
       "       0.02531646, 0.02542373, 0.02553191, 0.02564103, 0.02575107,\n",
       "       0.02586207, 0.02597403, 0.02608696, 0.02620087, 0.02631579,\n",
       "       0.02643172, 0.02654867, 0.02666667, 0.02678571, 0.02690583,\n",
       "       0.02702703, 0.02714932, 0.02727273, 0.02739726, 0.02752294,\n",
       "       0.02764977, 0.02777778, 0.02790698, 0.02803738, 0.02347418,\n",
       "       0.02358491, 0.02369668, 0.02380952, 0.02392344, 0.02403846,\n",
       "       0.02415459, 0.02427184, 0.0195122 , 0.01960784, 0.01970443,\n",
       "       0.01980198, 0.01492537, 0.015     , 0.01507538, 0.01515152,\n",
       "       0.01522843, 0.01530612, 0.01538462, 0.01546392, 0.01554404,\n",
       "       0.015625  , 0.01570681, 0.01578947, 0.01587302, 0.01595745,\n",
       "       0.01604278, 0.01612903, 0.01621622, 0.01630435, 0.01639344,\n",
       "       0.01098901, 0.01104972, 0.01111111, 0.01117318, 0.01123596,\n",
       "       0.01129944, 0.01136364, 0.01142857, 0.01149425, 0.01156069,\n",
       "       0.01162791, 0.00584795, 0.00588235, 0.00591716, 0.00595238,\n",
       "       0.00598802, 0.0060241 , 0.00606061, 0.00609756, 0.00613497,\n",
       "       0.00617284, 0.00621118, 0.00625   , 0.00628931, 0.00632911,\n",
       "       0.00636943, 0.00641026, 0.00645161, 0.00649351, 0.00653595,\n",
       "       0.00657895, 0.00662252, 0.00666667, 0.00671141, 0.00675676,\n",
       "       0.00680272, 0.00684932, 0.00689655, 0.00694444, 0.00699301,\n",
       "       0.00704225, 0.0070922 , 0.00714286, 0.00719424, 0.00724638,\n",
       "       0.00729927, 0.00735294, 0.00740741, 0.00746269, 0.0075188 ,\n",
       "       0.00757576, 0.00763359, 0.00769231, 0.00775194, 0.0078125 ,\n",
       "       0.00787402, 0.00793651, 0.008     , 0.00806452, 0.00813008,\n",
       "       0.00819672, 0.00826446, 0.00833333, 0.00840336, 0.00847458,\n",
       "       0.00854701, 0.00862069, 0.00869565, 0.00877193, 0.00884956,\n",
       "       0.00892857, 0.00900901, 0.00909091, 0.00917431, 0.00925926,\n",
       "       0.00934579, 0.00943396, 0.00952381, 0.00961538, 0.00970874,\n",
       "       0.00980392, 0.00990099, 0.01      , 0.01010101, 0.01020408,\n",
       "       0.01030928, 0.01041667, 0.01052632, 0.0106383 , 0.01075269,\n",
       "       0.01086957, 0.01098901, 0.01111111, 0.01123596, 0.01136364,\n",
       "       0.01149425, 0.01162791, 0.01176471, 0.01190476, 0.01204819,\n",
       "       0.01219512, 0.01234568, 0.0125    , 0.01265823, 0.01282051,\n",
       "       0.01298701, 0.01315789, 0.01333333, 0.01351351, 0.01369863,\n",
       "       0.01388889, 0.01408451, 0.01428571, 0.01449275, 0.01470588,\n",
       "       0.01492537, 0.01515152, 0.01538462, 0.015625  , 0.01587302,\n",
       "       0.01612903, 0.01639344, 0.01666667, 0.01694915, 0.01724138,\n",
       "       0.01754386, 0.01785714, 0.01818182, 0.01851852, 0.01886792,\n",
       "       0.01923077, 0.01960784, 0.02      , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "       0.        , 1.        ])"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.20288233 -2.20288233 -2.20288233 -2.20288233 -1.886992   -1.67446009\n",
      " -1.39338274 -1.29191496 -1.07049611 -1.03685624 -0.85022202 -0.76589362\n",
      " -0.75658388 -0.67849053 -0.6636646  -0.65950955 -0.60731244 -0.51514714\n",
      " -0.4916576  -0.46814636 -0.41664931 -0.36481261 -0.31444321 -0.25179382\n",
      " -0.20764888 -0.20195471 -0.18245475 -0.17493513 -0.10651349 -0.05986415\n",
      " -0.04314528 -0.0298297  -0.01930595 -0.01420751  0.00868229  0.01044209\n",
      "  0.0359878   0.05462911  0.06130867  0.08823436  0.13244776  0.13653954\n",
      "  0.14163357  0.14210526  0.15506513  0.16744092  0.19276852  0.19900146\n",
      "  0.25272167  0.2695222 ]\n",
      "[-1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1 -1\n",
      " -1 -1 -1 -1 -1 -1 -1 -1 -1 -1  1  1  1  1  1  1  1  1  1  1  1  1  1  1\n",
      "  1  1]\n",
      "[65. 92.  6. 53. 65. 16. 64. 65. 88. 16. 64. 42. 64. 64. 15.  6. 65. 90.\n",
      " 62. 42. 62. 16. 16. 64. 53. 92.  6. 16. 67. 64. 65. 16. 16. 88. 88. 65.\n",
      " 65. 65. 65. 65. 16. 90. 15. 65. 65. 42. 65. 16. 92. 95.]\n"
     ]
    }
   ],
   "source": [
    "dists = clf.decision_function(X_test)\n",
    "argsort = np.argsort(dists)\n",
    "y_test_classes = np.copy(test_X.values[:,-1])\n",
    "print(dists[argsort][:50])\n",
    "print(y_pred_test[argsort][:50])\n",
    "print(y_test_classes[argsort][:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMP MLP MERGE ANALYSIS\n",
    "\n",
    "gal_oof = pd.read_hdf('../level_1_preds/mlp_v8.27_galactic_0.3062_oof.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4044, 15)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gal_oof.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_oof = pd.read_hdf('../level_1_preds/mlp_v8.28_extra_1.1252_oof.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5523, 15)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extra_oof.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "_oof = pd.read_hdf('../level_1_preds/mlp_v8.20_0.8799_oof.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>object_id</th>\n",
       "      <th>mlp_v8.20_0.8799__6</th>\n",
       "      <th>mlp_v8.20_0.8799__15</th>\n",
       "      <th>mlp_v8.20_0.8799__16</th>\n",
       "      <th>mlp_v8.20_0.8799__42</th>\n",
       "      <th>mlp_v8.20_0.8799__52</th>\n",
       "      <th>mlp_v8.20_0.8799__53</th>\n",
       "      <th>mlp_v8.20_0.8799__62</th>\n",
       "      <th>mlp_v8.20_0.8799__64</th>\n",
       "      <th>mlp_v8.20_0.8799__65</th>\n",
       "      <th>mlp_v8.20_0.8799__67</th>\n",
       "      <th>mlp_v8.20_0.8799__88</th>\n",
       "      <th>mlp_v8.20_0.8799__90</th>\n",
       "      <th>mlp_v8.20_0.8799__92</th>\n",
       "      <th>mlp_v8.20_0.8799__95</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13</td>\n",
       "      <td>0.000141</td>\n",
       "      <td>0.020444</td>\n",
       "      <td>0.000004</td>\n",
       "      <td>0.766997</td>\n",
       "      <td>0.071683</td>\n",
       "      <td>0.000253</td>\n",
       "      <td>0.115005</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000097</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.000499</td>\n",
       "      <td>0.019538</td>\n",
       "      <td>0.000101</td>\n",
       "      <td>0.001759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14</td>\n",
       "      <td>0.022175</td>\n",
       "      <td>0.087481</td>\n",
       "      <td>0.001768</td>\n",
       "      <td>0.139899</td>\n",
       "      <td>0.327391</td>\n",
       "      <td>0.002960</td>\n",
       "      <td>0.019983</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>0.102005</td>\n",
       "      <td>0.004038</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.287927</td>\n",
       "      <td>0.000579</td>\n",
       "      <td>0.002974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17</td>\n",
       "      <td>0.002427</td>\n",
       "      <td>0.071703</td>\n",
       "      <td>0.007001</td>\n",
       "      <td>0.163758</td>\n",
       "      <td>0.211467</td>\n",
       "      <td>0.001264</td>\n",
       "      <td>0.152283</td>\n",
       "      <td>0.094780</td>\n",
       "      <td>0.007937</td>\n",
       "      <td>0.096456</td>\n",
       "      <td>0.002374</td>\n",
       "      <td>0.184011</td>\n",
       "      <td>0.000531</td>\n",
       "      <td>0.004006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>23</td>\n",
       "      <td>0.003023</td>\n",
       "      <td>0.015836</td>\n",
       "      <td>0.001396</td>\n",
       "      <td>0.073920</td>\n",
       "      <td>0.231724</td>\n",
       "      <td>0.001057</td>\n",
       "      <td>0.159054</td>\n",
       "      <td>0.009946</td>\n",
       "      <td>0.001073</td>\n",
       "      <td>0.309197</td>\n",
       "      <td>0.000219</td>\n",
       "      <td>0.191808</td>\n",
       "      <td>0.000210</td>\n",
       "      <td>0.001538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>34</td>\n",
       "      <td>0.002734</td>\n",
       "      <td>0.011857</td>\n",
       "      <td>0.000295</td>\n",
       "      <td>0.160961</td>\n",
       "      <td>0.379045</td>\n",
       "      <td>0.000308</td>\n",
       "      <td>0.073131</td>\n",
       "      <td>0.000142</td>\n",
       "      <td>0.004454</td>\n",
       "      <td>0.018110</td>\n",
       "      <td>0.000068</td>\n",
       "      <td>0.341364</td>\n",
       "      <td>0.000113</td>\n",
       "      <td>0.007417</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   object_id  mlp_v8.20_0.8799__6  mlp_v8.20_0.8799__15  mlp_v8.20_0.8799__16  \\\n",
       "0         13             0.000141              0.020444              0.000004   \n",
       "1         14             0.022175              0.087481              0.001768   \n",
       "2         17             0.002427              0.071703              0.007001   \n",
       "3         23             0.003023              0.015836              0.001396   \n",
       "4         34             0.002734              0.011857              0.000295   \n",
       "\n",
       "   mlp_v8.20_0.8799__42  mlp_v8.20_0.8799__52  mlp_v8.20_0.8799__53  \\\n",
       "0              0.766997              0.071683              0.000253   \n",
       "1              0.139899              0.327391              0.002960   \n",
       "2              0.163758              0.211467              0.001264   \n",
       "3              0.073920              0.231724              0.001057   \n",
       "4              0.160961              0.379045              0.000308   \n",
       "\n",
       "   mlp_v8.20_0.8799__62  mlp_v8.20_0.8799__64  mlp_v8.20_0.8799__65  \\\n",
       "0              0.115005              0.000251              0.000097   \n",
       "1              0.019983              0.000316              0.102005   \n",
       "2              0.152283              0.094780              0.007937   \n",
       "3              0.159054              0.009946              0.001073   \n",
       "4              0.073131              0.000142              0.004454   \n",
       "\n",
       "   mlp_v8.20_0.8799__67  mlp_v8.20_0.8799__88  mlp_v8.20_0.8799__90  \\\n",
       "0              0.003228              0.000499              0.019538   \n",
       "1              0.004038              0.000505              0.287927   \n",
       "2              0.096456              0.002374              0.184011   \n",
       "3              0.309197              0.000219              0.191808   \n",
       "4              0.018110              0.000068              0.341364   \n",
       "\n",
       "   mlp_v8.20_0.8799__92  mlp_v8.20_0.8799__95  \n",
       "0              0.000101              0.001759  \n",
       "1              0.000579              0.002974  \n",
       "2              0.000531              0.004006  \n",
       "3              0.000210              0.001538  \n",
       "4              0.000113              0.007417  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_oof.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}